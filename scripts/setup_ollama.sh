#!/bin/bash

echo "ğŸš€ Configurando Ollama para o projeto Micro-RAG"
echo ""

# Detectar sistema operacional
OS="$(uname -s)"
case "${OS}" in
    Linux*)     MACHINE=Linux;;
    Darwin*)    MACHINE=Mac;;
    *)          MACHINE="UNKNOWN:${OS}"
esac

# Verificar se Ollama estÃ¡ instalado
if ! command -v ollama &> /dev/null; then
    echo "âŒ Ollama nÃ£o estÃ¡ instalado."
    echo "ğŸ“¥ Instalando Ollama..."
    
    if [ "$MACHINE" = "Mac" ]; then
        echo "ğŸ Detectado macOS"
        
        # Verificar se Homebrew estÃ¡ instalado
        if command -v brew &> /dev/null; then
            echo "ğŸ“¦ Instalando via Homebrew..."
            brew install ollama
        else
            echo "âš ï¸  Homebrew nÃ£o encontrado."
            echo ""
            echo "ğŸ“¥ Por favor, instale o Ollama manualmente:"
            echo "   1. Acesse: https://ollama.com/download"
            echo "   2. Baixe o instalador para macOS"
            echo "   3. Abra o arquivo .dmg e arraste Ollama para Applications"
            echo "   4. Execute este script novamente apÃ³s a instalaÃ§Ã£o"
            echo ""
            echo "   Ou instale Homebrew primeiro:"
            echo "   /bin/bash -c \"\$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\""
            exit 1
        fi
    elif [ "$MACHINE" = "Linux" ]; then
        echo "ğŸ§ Detectado Linux"
        echo "ğŸ“¥ Instalando via script oficial..."
        curl -fsSL https://ollama.com/install.sh | sh
    else
        echo "âŒ Sistema operacional nÃ£o suportado: $MACHINE"
        echo "ğŸ“¥ Por favor, instale manualmente em: https://ollama.com/download"
        exit 1
    fi
    
    echo "âœ… Ollama instalado!"
    ollama --version
else
    echo "âœ… Ollama jÃ¡ estÃ¡ instalado"
    ollama --version
fi

echo ""
echo "ğŸ” Verificando se o servidor Ollama estÃ¡ rodando..."

# Verificar se servidor estÃ¡ rodando
if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
    echo "âœ… Servidor Ollama estÃ¡ rodando"
else
    echo "âš ï¸  Servidor Ollama nÃ£o estÃ¡ rodando"
    echo "ğŸ”„ Iniciando servidor Ollama em background..."
    ollama serve &
    sleep 3
    
    if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
        echo "âœ… Servidor iniciado com sucesso"
    else
        echo "âŒ Erro ao iniciar servidor. Tente manualmente: ollama serve"
        exit 1
    fi
fi

echo ""
echo "ğŸ“¦ Baixando modelos necessÃ¡rios..."
echo ""

# Baixar modelo de embeddings
echo "ğŸ“¥ Baixando nomic-embed-text (para embeddings)..."
ollama pull nomic-embed-text

# Baixar modelo de LLM (perguntar qual)
echo ""
echo "Escolha o modelo de LLM para geraÃ§Ã£o:"
echo "1) llama3.2 (leve e rÃ¡pido)"
echo "2) qwen2.5:7b (recomendado para portuguÃªs)"
echo "3) mistral (boa qualidade)"
echo "4) llama3.1:8b (maior, melhor qualidade)"
read -p "Escolha (1-4) [padrÃ£o: 1]: " choice

case $choice in
    2)
        echo "ğŸ“¥ Baixando qwen2.5:7b..."
        ollama pull qwen2.5:7b
        MODEL="qwen2.5:7b"
        ;;
    3)
        echo "ğŸ“¥ Baixando mistral..."
        ollama pull mistral
        MODEL="mistral"
        ;;
    4)
        echo "ğŸ“¥ Baixando llama3.1:8b..."
        ollama pull llama3.1:8b
        MODEL="llama3.1:8b"
        ;;
    *)
        echo "ğŸ“¥ Baixando llama3.2..."
        ollama pull llama3.2
        MODEL="llama3.2"
        ;;
esac

echo ""
echo "âœ… ConfiguraÃ§Ã£o concluÃ­da!"
echo ""
echo "ğŸ“ Modelos instalados:"
ollama list
echo ""
echo "ğŸ§ª Testando modelo de LLM..."
ollama run $MODEL "Responda apenas: OK" --verbose
echo ""
echo "âœ¨ Setup completo! Agora vocÃª pode usar Ollama no projeto."
echo ""
echo "ğŸ’¡ Dica: Configure o .env com:"
echo "   OLLAMA_LLM_MODEL=$MODEL"
echo "   OLLAMA_EMBEDDING_MODEL=nomic-embed-text"