# Micro-RAG API com Guardrails

API de Q&A (Question & Answer) baseada em RAG (Retrieval-Augmented Generation) que permite fazer perguntas sobre documentos indexados em um vector database, com mÃºltiplas camadas de seguranÃ§a (guardrails) e observabilidade completa.

## ğŸ¯ VisÃ£o Geral

Este projeto implementa uma API REST que expÃµe funcionalidades de RAG, permitindo:
- **Retrieval**: Buscar informaÃ§Ãµes relevantes de documentos indexados no vector database
- **Generation**: Gerar respostas contextualizadas usando LLM
- **Guardrails**: ProteÃ§Ã£o contra prompt injection, dados sensÃ­veis e conteÃºdo malicioso
- **Observabilidade**: Rastreamento de mÃ©tricas, custos e versionamento de prompts via Langfuse

## ğŸ› ï¸ Tecnologias

- **FastAPI** - Framework web para expor a API REST
- **Ollama** - LLM local (Llama 3.2) e embeddings (nomic-embed-text)
- **Qdrant** - Vector database para armazenar embeddings e documentos
- **Langfuse** - Observabilidade, tracing e versionamento de prompts
- **LangChain** - OrquestraÃ§Ã£o de RAG (document loading, chunking, retrieval)
- **Pytest** - Framework de testes (cobertura mÃ­nima: 70%)

## ğŸ—ï¸ Arquitetura e PrincÃ­pios de Design

### AbstraÃ§Ã£o de Providers

O projeto segue os princÃ­pios **SOLID**, especialmente:
- **Open/Closed Principle**: Novos providers podem ser adicionados sem modificar o cÃ³digo existente
- **Single Responsibility Principle**: Cada provider tem uma responsabilidade Ãºnica

**Providers DisponÃ­veis:**
- `EmbeddingProvider` - AbstraÃ§Ã£o para modelos de embedding (Ollama, OpenAI, etc.)
- `VectorStoreProvider` - AbstraÃ§Ã£o para vector databases (Qdrant, Pinecone, etc.)
- `LangfuseProvider` - Gerenciamento de observabilidade e prompts

### Sistema de Fallback

- **Prompts Locais**: Prompts versionados na pasta `src/prompts/` servem como fallback
- **Langfuse**: Versionamento principal via Langfuse Cloud (opcional)
- Se Langfuse nÃ£o estiver disponÃ­vel, o sistema usa automaticamente os prompts locais

## ğŸ“‹ PrÃ©-requisitos

- **Python 3.13+**
- **Docker** e **Docker Compose** (recomendado)
- **uv** - Gerenciador de pacotes Python (instalado automaticamente no Dockerfile)

## ğŸš€ Como Executar

### OpÃ§Ã£o 1: Com Docker (Recomendado)

1. **Inicie todos os serviÃ§os:**
   ```bash
   docker-compose up --build
   ```

2. **Baixe os modelos do Ollama:**
   ```bash
   docker exec -it ollama ollama pull llama3.2
   docker exec -it ollama ollama pull nomic-embed-text
   ```

3. **Inicialize a collection no Qdrant:**
   ```bash
   docker-compose exec api uv run python scripts/init_qdrant.py
   ```

4. **Ingira os documentos:**
   ```bash
   docker-compose exec api uv run python scripts/ingest.py
   ```

5. **Teste a API:**
   ```bash
   curl http://localhost:8000/api/health
   ```

### OpÃ§Ã£o 2: Sem Docker

1. **Instale as dependÃªncias:**
   ```bash
   uv sync
   ```

2. **Inicie o Qdrant:**
   ```bash
   docker run -d -p 6333:6333 -p 6334:6334 qdrant/qdrant:latest
   ```

3. **Inicie o Ollama:**
   ```bash
   docker run -d -p 11434:11434 ollama/ollama:latest
   ```

4. **Baixe os modelos:**
   ```bash
   ollama pull llama3.2
   ollama pull nomic-embed-text
   ```

5. **Inicialize a collection:**
   ```bash
   uv run python scripts/init_qdrant.py
   ```

6. **Ingira os documentos:**
   ```bash
   uv run python scripts/ingest.py
   ```

7. **Inicie a API:**
   ```bash
   uv run src/main.py
   ```

## ğŸ“ Estrutura do Projeto

```
.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/              # Rotas e schemas da API
â”‚   â”œâ”€â”€ clients/          # Clientes de alto nÃ­vel (retrieval, embedding)
â”‚   â”œâ”€â”€ core/             # ConfiguraÃ§Ãµes e tipos
â”‚   â”œâ”€â”€ ingestion/        # Carregamento e chunking de documentos
â”‚   â”œâ”€â”€ providers/        # AbstraÃ§Ãµes de providers (Ollama, Qdrant, Langfuse)
â”‚   â”œâ”€â”€ prompts/         # Prompts versionados (fallback local)
â”‚   â”œâ”€â”€ services/        # LÃ³gica de negÃ³cio (QA, Guardrails)
â”‚   â””â”€â”€ utils/           # FunÃ§Ãµes auxiliares
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ init_qdrant.py   # Cria collection no Qdrant
â”‚   â””â”€â”€ ingest.py        # Indexa documentos da pasta data/
â”œâ”€â”€ tests/               # Testes automatizados
â”œâ”€â”€ docs/                # DocumentaÃ§Ã£o (Arquitetura, Contratos, Testes)
â”œâ”€â”€ data/                # Documentos para indexaÃ§Ã£o
â””â”€â”€ docker-compose.yml   # OrquestraÃ§Ã£o de serviÃ§os
```

## ğŸ”§ ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente

Crie um arquivo `.env` na raiz do projeto (opcional):

```env
# Ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_LLM_MODEL=llama3.2
OLLAMA_EMBEDDING_MODEL=nomic-embed-text

# Qdrant
VECTOR_DB_URL=http://localhost:6333
VECTOR_DB_COLLECTION_NAME=rag_docs

# API
API_HOST=0.0.0.0
API_PORT=8000
LOG_LEVEL=INFO

# RAG Config
CHUNK_SIZE=800
CHUNK_OVERLAP=200
DEFAULT_TOP_K=5

# Langfuse (Opcional)
LANGFUSE_SECRET_KEY=sk-lf-...
LANGFUSE_PUBLIC_KEY=pk-lf-...
LANGFUSE_HOST=https://cloud.langfuse.com
```

## ğŸ“– Uso da API

### Health Check

```bash
curl http://localhost:8000/api/health
```

### Fazer uma Pergunta

```bash
curl -X POST "http://localhost:8000/api/v1/query" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Qual Ã© o horÃ¡rio de funcionamento?",
    "top_k": 5
  }'
```

### DocumentaÃ§Ã£o Interativa

- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

## ğŸ§ª Testes

### Executar Todos os Testes

```bash
uv run pytest
```

### Executar com Cobertura

```bash
uv run pytest --cov=src --cov-report=html
```

### Executar Teste EspecÃ­fico

```bash
uv run pytest tests/test_guardrails.py
```

**Cobertura mÃ­nima:** 70% (configurado em `pytest.ini`)

## ğŸ“š DocumentaÃ§Ã£o Adicional

- **[Arquitetura](docs/ARQUITETURA.md)** - VisÃ£o geral da arquitetura e fluxo do sistema
- **[Contratos](docs/CONTRATOS.md)** - EspecificaÃ§Ã£o completa da API
- **[Testes](docs/TESTES.md)** - Processo de testes e validaÃ§Ã£o manual

## ğŸ›¡ï¸ Guardrails

O sistema possui mÃºltiplas camadas de seguranÃ§a:

1. **ValidaÃ§Ã£o Regex**: Detecta padrÃµes conhecidos de prompt injection e dados sensÃ­veis
2. **ValidaÃ§Ã£o LLM**: Usa anÃ¡lise semÃ¢ntica para detectar intenÃ§Ã£o maliciosa

### AlteraÃ§oes opcionais na camada de enviroments
#### Utilizado:
| ParÃ¢metro | Valor Escolhido | Alternativas | Impacto |
|-----------|-----------------|--------------|---------|
| **Chunk Size** | 800 | 500 (menor contexto), 1200 (mais contexto) | 800 equilibra contexto e granularidade |
| **Overlap** | 200 (25%) | 100 (12.5%), 400 (50%) | 25% Ã© padrÃ£o da indÃºstria |
| **Top-k** | 5 | 3 (mais preciso), 10 (mais recall) | 5 balanceia precisÃ£o e cobertura |

### ğŸ“ˆ Resultados Observados

Com estes parÃ¢metros, o sistema alcanÃ§a:
- **LatÃªncia Total**: ~1.2-1.5s por requisiÃ§Ã£o
- **RelevÃ¢ncia**: Alta precisÃ£o nas respostas (chunks recuperados sÃ£o pertinentes)
- **Cobertura**: Perguntas complexas sÃ£o respondidas com mÃºltiplas fontes
- **Tamanho do Ãndice**: ~N/800 Ã— 1.25 chunks por documento (considerando overlap)

### ğŸ¯ Quando Ajustar os ParÃ¢metros

**Aumente o Chunk Size (1000-1200)** se:
- Documentos tÃªm parÃ¡grafos muito longos
- Conceitos sÃ£o complexos e precisam de mais contexto

**Diminua o Chunk Size (500-600)** se:
- Documentos sÃ£o muito estruturados (listas, tabelas)
- Precisa de maior granularidade na busca

### Lembre-se para o Top-k Ã© possÃ­el alterar na prÃ³pria requisicao 

**Aumente o Top-k (7-10)** se:
- Perguntas sÃ£o muito abertas ou exploratÃ³rias
- Precisa de mais diversidade de fontes

**Diminua o Top-k (3)** se:
- Perguntas sÃ£o muito especÃ­ficas
- LatÃªncia Ã© crÃ­tica

## ğŸ“Š Observabilidade

- **Logs Estruturados**: Logging com Rich para output formatado
- **Langfuse Tracing**: Rastreamento automÃ¡tico de requisiÃ§Ãµes, mÃ©tricas e custos
- **MÃ©tricas Detalhadas**: LatÃªncia, tokens, custos por requisiÃ§Ã£o


## ğŸ“ LicenÃ§a

Este projeto Ã© um teste tÃ©cnico.

---

**Desenvolvido por Raphael Rissoli**
