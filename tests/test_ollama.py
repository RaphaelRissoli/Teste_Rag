#!/usr/bin/env python3
"""
Script de teste para verificar se Ollama estÃ¡ configurado corretamente
"""
import sys
from pathlib import Path

# Adicionar src ao path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

import requests
from langchain_ollama import ChatOllama

def test_ollama_server():
    """Testa se o servidor Ollama estÃ¡ rodando"""
    print("ğŸ” Testando servidor Ollama...")
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get("models", [])
            print(f"âœ… Servidor Ollama estÃ¡ rodando")
            print(f"ğŸ“¦ Modelos instalados: {len(models)}")
            for model in models:
                print(f"   - {model.get('name', 'unknown')}")
            return True
        else:
            print(f"âŒ Servidor respondeu com cÃ³digo {response.status_code}")
            return False
    except requests.exceptions.ConnectionError:
        print("âŒ NÃ£o foi possÃ­vel conectar ao servidor Ollama")
        print("ğŸ’¡ Execute: ollama serve")
        return False
    except Exception as e:
        print(f"âŒ Erro: {e}")
        return False

def test_llm():
    """Testa geraÃ§Ã£o de texto com LLM"""
    print("\nğŸ§ª Testando geraÃ§Ã£o de texto...")
    try:
        llm = ChatOllama(
            model="llama3.2",
            base_url="http://localhost:11434",
            temperature=0.3
        )
        
        response = llm.invoke("Responda apenas: OK")
        print(f"âœ… LLM funcionando!")
        print(f"ğŸ“ Resposta: {response.content[:100]}")
        return True
    except Exception as e:
        print(f"âŒ Erro ao testar LLM: {e}")
        print("ğŸ’¡ Verifique se o modelo estÃ¡ instalado: ollama list")
        return False

def test_embeddings():
    """Testa geraÃ§Ã£o de embeddings"""
    print("\nğŸ§ª Testando embeddings...")
    try:
        response = requests.post(
            "http://localhost:11434/api/embeddings",
            json={
                "model": "nomic-embed-text",
                "prompt": "Teste de embedding"
            },
            timeout=30
        )
        
        if response.status_code == 200:
            embedding = response.json().get("embedding", [])
            print(f"âœ… Embeddings funcionando!")
            print(f"ğŸ“Š DimensÃµes: {len(embedding)}")
            return True
        else:
            print(f"âŒ Erro: {response.status_code}")
            print(response.text)
            return False
    except Exception as e:
        print(f"âŒ Erro ao testar embeddings: {e}")
        print("ğŸ’¡ Verifique se nomic-embed-text estÃ¡ instalado: ollama pull nomic-embed-text")
        return False

def main():
    print("=" * 50)
    print("ğŸ§ª Teste de ConfiguraÃ§Ã£o Ollama")
    print("=" * 50)
    print()
    
    server_ok = test_ollama_server()
    
    if not server_ok:
        print("\nâŒ Servidor nÃ£o estÃ¡ rodando. Execute: ollama serve")
        return
    
    embeddings_ok = test_embeddings()
    llm_ok = test_llm()
    
    print("\n" + "=" * 50)
    if embeddings_ok and llm_ok:
        print("âœ… Todos os testes passaram!")
        print("âœ¨ Ollama estÃ¡ pronto para uso")
    else:
        print("âš ï¸  Alguns testes falharam")
        print("ğŸ’¡ Verifique os modelos instalados: ollama list")
    print("=" * 50)

if __name__ == "__main__":
    main()